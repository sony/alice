{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting CUDA device 2 with 47642 MiB free memory and 6% utilization\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if '_cwd_set' not in locals(): locals()['_cwd_set'] = os.chdir(os.path.dirname(os.getcwd()))\n",
    "import omnifig as fig\n",
    "fig.initialize()\n",
    "from src.jimports import *\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
    "# for all stats at once\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from src.util import set_default_device, repo_root, data_root\n",
    "from src.dataset import RawCOCO, SimpleCOCO, RawCOCOCaptions, COCOCaptions, COCO, MNIST\n",
    "device = set_default_device();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(COCO[25014](index, text_features, label, image_id, caption_id, caption, image, image_features),\n",
       " 5000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fullddata = COCO(eval_split=-0.1)\n",
    "syscfg = fig.create_config('h/ws2')\n",
    "syscfg.silent=True\n",
    "fullddata = COCO(eval_split=None, split='val', dataroot=syscfg.pull('dataroot'))\n",
    "fullddata.prepare(device=device);\n",
    "fiids = fullddata.get_image_id(np.arange(fullddata.size))\n",
    "idmap = {}\n",
    "for i, im in enumerate(fiids.tolist()):\n",
    "    idmap.setdefault(im, []).append(i)\n",
    "picks = np.array([options[0] for options in idmap.values()])\n",
    "fullddata, len(picks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = '/data/felix/cache/checkpoints/vae128_coco_20250116_222421/ckpt_100000'\n",
    "loc = '/data/felix/cache/checkpoints/ae128_coco_20250114_230254/ckpt_020000/'\n",
    "loc = '/data/felix/cache/checkpoints/sae128_coco_20250118_213955/ckpt_100000'\n",
    "loc = '/ssd/felix/cache/checkpoints/sae512_coco_20250124_164611/ckpt_100000/'\n",
    "loc = Path(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = fig.create_config(*'norm a/wide m/ced-man d/coco-img'.split())\n",
    "cfg = fig.create_config(str(loc / 'config.yaml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/workspace/clones/omni-learn/omnilearn/machines.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "trainer = cfg.pull('trainer', silent=True)\n",
    "model = trainer.model\n",
    "traindataset = cfg.pull('dataset', silent=True)\n",
    "valdataset = traindataset.as_eval()\n",
    "traindataset.prepare(device=device);\n",
    "valdataset.prepare(device=device)\n",
    "system = Structured(traindataset, *trainer.gadgetry())\n",
    "system.mechanize() # sync for gears and spaces\n",
    "mech = system.mechanics()\n",
    "model.prepare(device=device);\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.load_checkpoint(path=loc.joinpath('model'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1836544"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in trainer.intervention.module.parameters())\n",
    "trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_img_ids = valdataset.get_image_id(np.arange(valdataset.size))\n",
    "# safe_ids = set(fiids.tolist()).intersection(set(val_img_ids.tolist()))\n",
    "# len(val_img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_img_ids = valdataset.get_image_id(np.arange(valdataset.size))\n",
    "# fiids = fullddata.get_image_id(np.arange(fullddata.size))\n",
    "# fiid_map = {fiid: i for i, fiid in enumerate(fiids)}\n",
    "# # inds = np.array([fiid_map[fiid] for fiid in safe_ids])\n",
    "# # inds.sort()\n",
    "# # save_json(inds.tolist(), 'save_inds.json')\n",
    "# inds = load_json('save_inds.json')\n",
    "# inds = np.array(inds)\n",
    "# len(inds), inds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4135"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valfulldata = COCO(eval_split=-0.1)\n",
    "# fullddata = COCO(eval_split=None, split='val')\n",
    "valfulldata.prepare(device=device);\n",
    "fiids = valfulldata.get_image_id(np.arange(valfulldata.size))\n",
    "valmap = {}\n",
    "for i, im in enumerate(fiids.tolist()):\n",
    "    valmap.setdefault(im, []).append(i)\n",
    "val_img_ids = set(valdataset.get_image_id(np.arange(valdataset.size)).tolist())\n",
    "valpicks = np.array([options[0] for key, options in valmap.items() if key in val_img_ids])\n",
    "len(valpicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Context({index}, {text_features}, {label}, {image_id}, {caption_id}, {caption}, {image}, {image_features}, {size})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inds = picks\n",
    "batch = Context(fullddata, DictGadget({'index': inds, 'size': len(inds)}))\n",
    "inds = valpicks\n",
    "valbatch = Context(valfulldata, DictGadget({'index': inds, 'size': len(inds)}))\n",
    "valbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 128, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valemb = model.encode(valbatch['image_features'])\n",
    "lbls = valbatch['label']\n",
    "valemb.shape, lbls.shape\n",
    "# lbl = 40\n",
    "# lbls[:, lbl].sum()\n",
    "# pos = emb[lbls[:, lbl]]\n",
    "# proj = pos.T @ pos / lbls[:, lbl].sum()\n",
    "# proj.shape\n",
    "projs = []\n",
    "for lbl in range(lbls.shape[1]):\n",
    "    pos = valemb[lbls[:, lbl]]\n",
    "    proj = pos.T @ pos / lbls[:, lbl].sum()\n",
    "    projs.append(proj)\n",
    "projs = torch.stack(projs)\n",
    "projs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 80])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "z = model.encode(batch['image_features'])\n",
    "y = batch['label']\n",
    "score = model.predict(z)\n",
    "# yp = score.sub(thrs.unsqueeze(0)).ge(0)\n",
    "score.shape\n",
    "\n",
    "ic = 2\n",
    "y[:,ic].float().mean()\n",
    "\n",
    "dz = z @ projs[ic]\n",
    "dz = z + (-1) ** y[:,ic].unsqueeze(-1) * dz\n",
    "dz = dz / dz.norm(dim=-1, keepdim=True)\n",
    "dz.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80,), (80, 80))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_results = []\n",
    "intv_results = []\n",
    "\n",
    "obs_score = model.predict(z).cpu().numpy()\n",
    "\n",
    "for ic in tqdm(range(y.shape[1])):\n",
    "    dz = z @ projs[ic]\n",
    "    dz = z + (-1) ** y[:,ic].unsqueeze(-1) * dz\n",
    "    dz = dz / dz.norm(dim=-1, keepdim=True)\n",
    "    intv_score = model.predict(dz).cpu().numpy()\n",
    "    obs_results.append(roc_auc_score(gt, obs_score[:, i]))\n",
    "    intv_row = []\n",
    "    for i in range(y.shape[1]):\n",
    "        gt = y[:, i].cpu().numpy()\n",
    "        if i == ic:\n",
    "            gt = ~gt\n",
    "        intv_row.append(roc_auc_score(gt, intv_score[:, i]))\n",
    "    intv_results.append(intv_row)\n",
    "obs_results = np.array(obs_results)\n",
    "intv_results = np.array(intv_results)\n",
    "obs_results.shape, intv_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  9.1324,  -9.0957, -11.5980, -13.6753, -15.8401, -17.5224, -11.4130,\n",
       "        -10.6889,  -9.6972, -12.5531, -19.4821, -16.7362, -18.4169,  -8.7252,\n",
       "        -11.3438,  -8.1346,  -8.7017,  -8.9816, -15.1360, -16.9146, -14.9189,\n",
       "        -13.8886, -17.9677, -13.7450,  -7.1458, -11.8310,  -5.7632,  -7.5476,\n",
       "        -10.2915, -14.5164, -22.3432, -23.1013, -12.3706, -22.9805, -13.9289,\n",
       "        -18.5452, -22.8026, -20.0341, -11.0255,  -0.3999,  -6.2802,  -0.1928,\n",
       "         -8.1223,  -6.1516,  -6.1214,  -2.8473, -10.8983,  -6.5904, -11.2488,\n",
       "         -7.8607, -17.4817, -12.2376, -13.9882, -16.8319, -15.4528, -13.8059,\n",
       "         -5.0010,  -7.1793,   0.6025,  -6.3523,  -8.7216,  -0.9808,  -8.0420,\n",
       "         -6.2589, -13.6159,  -7.4622, -11.5423,  -1.2052,  -9.2549,  -9.9871,\n",
       "        -12.3797,   5.3217, -10.4644,  -2.8025,  -4.4475,  -0.2001,  -5.8738,\n",
       "         -7.3922,  -3.4281,  -1.3222], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(emb)[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = (projs[-4] - projs[-5]) @ emb[idx]\n",
    "new = new / new.norm()\n",
    "new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  3.9024, -10.6718,  -6.7774, -13.5819,  -5.4185,  -5.7317, -10.2014,\n",
       "          -8.8080, -14.0354,  -9.1711, -16.1837,  -8.8150, -13.6464, -10.6974,\n",
       "          -6.5013, -16.7903, -10.8953, -11.2102, -15.7333,  -9.0341, -13.8807,\n",
       "         -17.1458, -19.8320, -17.6132,  -6.9916, -12.8268,  -8.4250, -12.4215,\n",
       "         -12.9349,  -9.5340, -12.5550, -14.9554, -10.9486,  -7.8032, -15.0483,\n",
       "         -12.4928, -14.6519, -15.7950, -19.9726,  -8.9246, -21.3170, -14.5983,\n",
       "          -9.4339,  -6.3230, -10.8002,  -8.1817, -10.1597, -11.4045,  -9.9101,\n",
       "         -14.0311, -13.2979,  -7.0450, -10.4698, -13.5920, -16.6083,  -6.0683,\n",
       "         -13.4473, -19.2462, -16.3298, -23.2252, -12.8410, -21.5716, -14.6681,\n",
       "         -14.6640, -12.4886, -14.2852, -11.2389,  -8.4424, -14.2416, -15.9075,\n",
       "          -8.9048, -10.5014, -15.6897, -11.2378, -14.1076, -27.9013,   0.8805,\n",
       "         -14.6172, -19.6178, -15.6648]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(new.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4135, 80])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(emb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9789, device='cuda:0', dtype=torch.float64),\n",
       " tensor(-3.2697, device='cuda:0'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(emb)\n",
    "i = 0\n",
    "thrs = []\n",
    "aucs = []\n",
    "for i in range(pred.shape[1]):\n",
    "    fpr, tpr, thresholds = roc_curve(lbls[:,i].cpu().numpy(), pred[:,i].cpu().numpy())\n",
    "    auc = roc_auc_score(lbls[:,i].cpu().numpy(), pred[:,i].cpu().numpy())\n",
    "    thr = thresholds[(tpr-fpr).argmax()]\n",
    "    thrs.append(thr)\n",
    "    aucs.append(auc)\n",
    "thrs = torch.as_tensor(thrs).to(device)\n",
    "vaucs = torch.as_tensor(aucs).to(device)\n",
    "vaucs.mean(), thrs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 80])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = model.encode(batch['image_features'])\n",
    "y = batch['label']\n",
    "score = model.predict(z)\n",
    "yp = score.sub(thrs.unsqueeze(0)).ge(0)\n",
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9674, device='cuda:2', dtype=torch.float64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "aucs = []\n",
    "for i in range(pred.shape[1]):\n",
    "    auc = roc_auc_score(y[:,i].cpu().numpy(), score[:,i].cpu().numpy())\n",
    "    aucs.append(auc)\n",
    "aucs = torch.as_tensor(aucs).to(device)\n",
    "aucs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.2764505119453925)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y[:,idx].cpu().numpy(), yp[:,idx].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 3])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs = []\n",
    "for idx in range(y.shape[1]):\n",
    "    p, r, f, _ = precision_recall_fscore_support(y[:,idx].cpu().numpy(), yp[:,idx].cpu().numpy())\n",
    "    outs.append((p[1], r[1], f[1]))\n",
    "outs = torch.as_tensor(outs).to(device)\n",
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9073, 0.2378, 0.5214, 0.5544, 0.8257, 0.5926, 0.6576, 0.4215, 0.6176,\n",
       "        0.3743, 0.2765, 0.1743, 0.1308, 0.2748, 0.2030, 0.7054, 0.3593, 0.5011,\n",
       "        0.6087, 0.4420, 0.9438, 0.7500, 0.9390, 0.8919, 0.3023, 0.3384, 0.3568,\n",
       "        0.4231, 0.2056, 0.4862, 0.8679, 0.4653, 0.5704, 0.7330, 0.6690, 0.8597,\n",
       "        0.6244, 0.8581, 0.9174, 0.4437, 0.3298, 0.4681, 0.3543, 0.4094, 0.3725,\n",
       "        0.4373, 0.4373, 0.2147, 0.4462, 0.2431, 0.4769, 0.4409, 0.2479, 0.7949,\n",
       "        0.3661, 0.4580, 0.5067, 0.3570, 0.1854, 0.4668, 0.5943, 0.6712, 0.5326,\n",
       "        0.5668, 0.6884, 0.5345, 0.4138, 0.3149, 0.1889, 0.4283, 0.2286, 0.6654,\n",
       "        0.4093, 0.3803, 0.2857, 0.3659, 0.1366, 0.4501, 0.0613, 0.1299],\n",
       "       device='cuda:2', dtype=torch.float64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[:, -1]#.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.86018117, 0.95002032]),\n",
       " array([0.94668401, 0.86817675]),\n",
       " array([0.90136195, 0.9072565 ]),\n",
       " array([2307, 2693]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
