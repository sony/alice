{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting CUDA device 0 with 48386 MiB free memory and 0% utilization\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if '_cwd_set' not in locals(): locals()['_cwd_set'] = os.chdir(os.path.dirname(os.getcwd()))\n",
    "import omnifig as fig\n",
    "fig.initialize()\n",
    "from src.jimports import *\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score, precision_recall_curve\n",
    "# for all stats at once\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from src.util import set_default_device, repo_root, data_root\n",
    "from src.interventions import ClassLevelLabelIntervention\n",
    "from src.dataset import RawCOCO, SimpleCOCO, RawCOCOCaptions, COCOCaptions, COCO, MNIST\n",
    "from src.interventions import ClassLevelModuleIntervention, ClassLevelCentroidIntervention, ClassLevelLinearIntervention\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from contextlib import redirect_stdout\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.optimize import root_scalar\n",
    "from scipy.stats import norm\n",
    "from src.baselines import ConceptAlgebra\n",
    "from matplotlib import patches as mpatches\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "device = set_default_device();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def roc_threshold(positives, negatives):\n",
    "    \"\"\"\n",
    "    Compute the optimal threshold using the ROC curve.\n",
    "    \n",
    "    Args:\n",
    "        positives: Array of data points from the positive class.\n",
    "        negatives: Array of data points from the negative class.\n",
    "\n",
    "    Returns:\n",
    "        Optimal threshold (float) based on the ROC curve.\n",
    "    \"\"\"\n",
    "    y_true = np.concatenate([np.ones(len(positives)), np.zeros(len(negatives))])\n",
    "    y_scores = np.concatenate([positives, negatives])\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    return thresholds[optimal_idx]\n",
    "\n",
    "def bayes_threshold(positives, negatives):\n",
    "    \"\"\"\n",
    "    Compute the Bayes threshold using the intersection of Gaussian PDFs.\n",
    "    Assumes both distributions are Gaussian.\n",
    "\n",
    "    Args:\n",
    "        positives: Array of data points from the positive class.\n",
    "        negatives: Array of data points from the negative class.\n",
    "\n",
    "    Returns:\n",
    "        Bayes threshold (float).\n",
    "    \"\"\"\n",
    "    m1, s1 = np.mean(positives), np.std(positives)\n",
    "    m0, s0 = np.mean(negatives), np.std(negatives)\n",
    "\n",
    "    def pdf_diff(x):\n",
    "        return norm.pdf(x, loc=m1, scale=s1) - norm.pdf(x, loc=m0, scale=s0)\n",
    "\n",
    "    result = root_scalar(pdf_diff, bracket=[m0, m1], method='brentq')\n",
    "    return result.root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fullddata = COCO(eval_split=-0.1)\n",
    "syscfg = fig.create_config('h/ws2')\n",
    "syscfg.silent=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc = '/data/felix/cache/checkpoints/vae128_coco_20250116_222421/ckpt_100000'\n",
    "# loc = '/data/felix/cache/checkpoints/ae128_coco_20250114_230254/ckpt_020000/'\n",
    "# loc = '/data/felix/cache/checkpoints/sae128_coco_20250118_213955/ckpt_100000'\n",
    "# loc = '/ssd/felix/cache/checkpoints/sae512_coco_20250128_121655/ckpt_010000'\n",
    "# loc = Path(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = fig.create_config(*'h/ws2 a/wide norm m/ced d/coco-img'.split(), **{'latent-dim': 512, 'classifier.dropout': 0.1})\n",
    "# trainer = cfg.pull('trainer', silent=True)\n",
    "# model = trainer.model\n",
    "# traindataset = cfg.pull('dataset', silent=True)\n",
    "# valdataset = traindataset.as_eval()\n",
    "# traindataset.prepare(device=device);\n",
    "# valdataset.prepare(device=device)\n",
    "# system = Structured(traindataset, *trainer.gadgetry())\n",
    "# system.mechanize() # sync for gears and spaces\n",
    "# mech = system.mechanics()\n",
    "# model.prepare(device=device);\n",
    "# for p in model.parameters():\n",
    "#     p.requires_grad = False\n",
    "# model.load_checkpoint(path=loc.joinpath('model'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img 2 models: img2-module, img2-fromimg-module\n"
     ]
    }
   ],
   "source": [
    "ckptroot = '/ssd/felix/cache/checkpoints/'\n",
    "\n",
    "textckpts = {\n",
    "    # 'sae512_cococap_20250129_105634': 'concept-text', # vital-deluge-11\n",
    "    # 'sae512_cococap_20250125_123713': 'text1-module', # treasured-sunset-9\n",
    "    'sae512_cococap_20250125_114125': 'text2-module', # glad-butterfly-15\n",
    "    'sae512_cococap_20250129_115412': 'text2-fromtext-module', # fragrant-armadillo-23\n",
    "\n",
    "    # 'sae512_cococap_20250129_114401': 'text1-centroid', # dutiful-shape-12\n",
    "    # 'sae512_cococap_20250129_114559': 'text1-affine', # zany-sea-13\n",
    "    \n",
    "    # 'sae512_cococap_20250129_114725': 'sem-text1-module', # crimson-salad-14\n",
    "    # 'sae512_cococap_20250129_115122': 'sem-text1-centroid', # usual-cloud-16\n",
    "    # 'sae512_cococap_20250129_115155': 'sem-text1-affine', # usual-serenity-17\n",
    "}\n",
    "\n",
    "imgckpts = {\n",
    "    # 'sae512_coco_20250128_121655': 'concept-img', # light-wood-123\n",
    "    # 'sae512_coco_20250124_164611': 'img1-module', # super-deluge-104\n",
    "    'sae512_coco_20250129_114055': 'img2-module', # distinctive-disco-3\n",
    "    'sae512_coco_20250125_234035': 'img2-fromimg-module', # serene-leaf-1\n",
    "\n",
    "    # 'sae512_coco_20250129_112704': 'img1-centroid', # vocal-wildflower-124\n",
    "    # 'sae512_coco_20250129_112805': 'img1-affine', # scarlet-flower-125\n",
    "    \n",
    "    # 'sae512_coco_20250129_112925': 'sem-img1-module', # jumping-sun-126\n",
    "    # 'sae512_coco_20250129_113004': 'sem-img1-centroid', # kind-darkness-127\n",
    "    # 'sae512_coco_20250129_113041': 'sem-img1-affine', # rare-fog-128\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "modality = 'img'\n",
    "# modality = 'text'\n",
    "assert modality in ['img', 'text']\n",
    "if modality == 'img':\n",
    "    confname = 'd/coco-img'\n",
    "    ckpts = imgckpts\n",
    "    ckptname = 'ckpt_020000'\n",
    "    ckptname = 'ckpt_080000'\n",
    "    # ckptname = 'ckpt_100000'\n",
    "else:\n",
    "    confname = 'd/coco-cap'\n",
    "    ckpts = textckpts\n",
    "    ckptname = 'ckpt_080000'\n",
    "    # ckptname = 'ckpt_100000'\n",
    "\n",
    "assert all(Path(ckptroot).joinpath(k,ckptname).exists() for k in ckpts.keys()), f'missing: {\", \".join(k for k in ckpts.keys() if not Path(ckptroot).joinpath(k,ckptname).exists())}'\n",
    "print(f'{modality} {len(ckpts)} models: {\", \".join(ckpts.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SimpleCOCO[106459](index, image, image_id, observation, instances, label),\n",
       " SimpleCOCO[11828](index, image, image_id, observation, instances, label),\n",
       " 11828)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindataset = fig.create_config('h/ws2', confname, split='train', eval_split=0.1).pull('dataset', silent=True)\n",
    "valdataset = traindataset.as_eval().prepare(device=device)\n",
    "# traindataset.prepare(device=device);\n",
    "# todo prepare validation samples\n",
    "num_classes = valdataset.label_space.n\n",
    "class_names = valdataset.label_space.class_names\n",
    "\n",
    "fiids = valdataset.get_image_id(np.arange(valdataset.size))\n",
    "valmap = {}\n",
    "for i, im in enumerate(fiids.tolist()):\n",
    "    valmap.setdefault(im, []).append(i)\n",
    "valpicks = np.array([options[0] for key, options in valmap.items()])\n",
    "traindataset, valdataset, len(valpicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78867ab4aed44626a202fc7195731361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/workspace/clones/omni-learn/omnilearn/machines.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(path, map_location='cpu')\n",
      "/home/felix/workspace/remote/src/interventions.py:337: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self._load_path)\n",
      "/home/felix/workspace/remote/src/interventions.py:291: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "def load_model(name):\n",
    "    path = Path(ckptroot).joinpath(name, ckptname)\n",
    "\n",
    "    cfg = fig.create_config(str(path/'config.yaml'), **{'use-wandb': False, 'device': device})\n",
    "    cfg.silent=True\n",
    "\n",
    "    valdataset._mechanics = None\n",
    "    with redirect_stdout(None):\n",
    "        trainer = cfg.pull('trainer', silent=True)\n",
    "        trainer.setup(valdataset, device=device)\n",
    "        valdataset._mechanics = None\n",
    "\n",
    "    model = trainer.model\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    model.load_checkpoint(path=path.joinpath('model'))\n",
    "    model.eval()\n",
    "\n",
    "    intv = getattr(trainer, 'intervention', None)\n",
    "    if intv is None:\n",
    "        assert ckpts[name].startswith('concept-'), f'no intervention for {name} ({ckpts[name]})'\n",
    "        intv = ConceptAlgebra(as_delta=False)\n",
    "        trainer.intervention = intv\n",
    "    else:\n",
    "        for p in intv.parameters():\n",
    "            p.requires_grad = False\n",
    "        intv.load_checkpoint(path=path.joinpath('interventions'))\n",
    "        intv.eval()\n",
    "        intv.to(device)\n",
    "\n",
    "    trainer.ckptname = name\n",
    "    trainer.ident = ckpts[name]\n",
    "    return trainer\n",
    "trainers = [load_model(name) for name in tqdm(ckpts.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Context(index, observation, {image}, {image_id}, {instances}, {label}, {size}),\n",
       " torch.Size([11828, 768]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vpicks = valpicks#[:10000]\n",
    "valbatch = Context(valdataset, DictGadget({'size': len(vpicks), 'index': vpicks}))\n",
    "valbatch, valbatch['observation'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44685e4222c4accb65cd0d21183cc13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                  val-avg-auc    val-min-auc    val-avg-f1    val-min-f1\n",
      "-------------------  -------------  -------------  ------------  ------------\n",
      "img2-module               0.940402       0.81844       0.369796     0.0155678\n",
      "img2-fromimg-module       0.942137       0.803919      0.380324     0.0578662\n"
     ]
    }
   ],
   "source": [
    "# compute classification thresholds\n",
    "for tr in tqdm(trainers):\n",
    "    vz = tr.model.encode(valbatch['observation'])\n",
    "    if isinstance(tr.intervention, ConceptAlgebra):\n",
    "        tr.intervention.extract_projections(vz, valbatch['label'])\n",
    "    \n",
    "    vscore = tr.model.predict(vz)\n",
    "    vy = valbatch['label']\n",
    "\n",
    "    thresholds = []\n",
    "    aucs = []\n",
    "    f1s = []\n",
    "    for ci in range(num_classes):        \n",
    "        points = vscore[:, ci].cpu().numpy()\n",
    "        gtlabels = vy[:, ci].cpu().numpy()\n",
    "\n",
    "        threshold = roc_threshold(points[gtlabels], points[~gtlabels])\n",
    "        # threshold = bayes_threshold(points[gtlabels], points[~gtlabels])\n",
    "        thresholds.append(threshold)\n",
    "        f1 = f1_score(gtlabels, points > threshold)\n",
    "        f1s.append(f1)\n",
    "        aucs.append(roc_auc_score(gtlabels, points))\n",
    "    \n",
    "    tr.aucs = torch.as_tensor(aucs).to(device)\n",
    "    tr.f1s = torch.as_tensor(f1s).to(device)\n",
    "    tr.thresholds = torch.as_tensor(thresholds).to(device)\n",
    "print(tabulate([[t.ident, t.aucs.mean().item(), t.aucs.min().item(), t.f1s.mean().item(), t.f1s.min().item()] for t in trainers], headers=['model', 'val-avg-auc', 'val-min-auc', 'val-avg-f1', 'val-min-f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation class rankings\n",
      "model    img2-module            img2-fromimg-module\n",
      "-------  ---------------------  ---------------------\n",
      "top-auc  zebra           0.998  zebra     0.998\n",
      "         giraffe         0.997  airplane  0.995\n",
      "         baseball glove  0.995  giraffe   0.994\n",
      "         baseball bat    0.995  bear      0.993\n",
      "         elephant        0.993  elephant  0.993\n",
      "         --------------  -----  --------  -----\n",
      "low-auc  bench         0.856    bench         0.845\n",
      "         potted plant  0.854    cup           0.833\n",
      "         cell phone    0.848    potted plant  0.82\n",
      "         handbag       0.823    handbag       0.818\n",
      "         backpack      0.818    backpack      0.804\n",
      "         ------------  -----    ------------  -----\n",
      "top-f1   tennis racket  0.865   person         0.877\n",
      "         person         0.853   tennis racket  0.83\n",
      "         elephant       0.813   train          0.79\n",
      "         giraffe        0.789   airplane       0.788\n",
      "         airplane       0.697   elephant       0.757\n",
      "         -------------  -----   -------------  -----\n",
      "low-f1   apple          0.116   stop sign   0.135\n",
      "         scissors       0.069   toothbrush  0.128\n",
      "         parking meter  0.054   scissors    0.076\n",
      "         hair drier     0.033   hair drier  0.065\n",
      "         toaster        0.016   toaster     0.058\n",
      "         -------------  -----   ----------  -----\n"
     ]
    }
   ],
   "source": [
    "num = 5\n",
    "tbl = []\n",
    "for tr in trainers:\n",
    "    order = np.argsort(tr.aucs.cpu().numpy())\n",
    "    top_auc = [f'{class_names[i]}:{tr.aucs[i].item():.3f}' for i in order[:-num-1:-1]]\n",
    "    low_auc = [f'{class_names[i]}:{tr.aucs[i].item():.3f}' for i in order[num-1::-1]]\n",
    "\n",
    "    order = np.argsort(tr.f1s.cpu().numpy())\n",
    "    top_f1 = [f'{class_names[i]}:{tr.f1s[i].item():.3f}' for i in order[:-num-1:-1]]\n",
    "    low_f1 = [f'{class_names[i]}:{tr.f1s[i].item():.3f}' for i in order[num-1::-1]]\n",
    "\n",
    "    # tbl.append([tr.ident, '\\n'.join(top_auc), '\\n'.join(low_auc), '\\n'.join(top_f1), '\\n'.join(low_f1)])\n",
    "    tbl.append([tr.ident, \n",
    "                '\\n'.join(tabulate([x.split(':') for x in top_auc]).split('\\n')[1:]), \n",
    "                '\\n'.join(tabulate([x.split(':') for x in low_auc]).split('\\n')[1:]),\n",
    "                '\\n'.join(tabulate([x.split(':') for x in top_f1]).split('\\n')[1:]),\n",
    "                '\\n'.join(tabulate([x.split(':') for x in low_f1]).split('\\n')[1:])])\n",
    "tbl = [['model', 'top-auc', 'low-auc', 'top-f1', 'low-f1']] + tbl\n",
    "ttbl = list(zip(*tbl))\n",
    "print('validation class rankings')\n",
    "print(tabulate(ttbl[1:], headers=ttbl[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SimpleCOCO[5000](index, image, image_id, observation, instances, label), 5000)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdataset = fig.create_config('h/ws2', confname, split='val').pull('dataset', silent=True).prepare(device=device)\n",
    "fiids = testdataset.get_image_id(np.arange(testdataset.size))\n",
    "idmap = {}\n",
    "for i, im in enumerate(fiids.tolist()):\n",
    "    idmap.setdefault(im, []).append(i)\n",
    "picks = np.array([options[0] for options in idmap.values()])\n",
    "testdataset, len(picks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Context(size, {index}, {image}, {image_id}, {observation}, {instances}, {label}),\n",
       " 5000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = Context(testdataset, DictGadget({'size': len(picks), 'index': picks}))\n",
    "batch, batch['size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "gti = ClassLevelLabelIntervention()\n",
    "def compute_interventional_metrics(y, yhat, iy, iyhat):\n",
    "    valid = y == yhat\n",
    "    changed_mask = iy != y\n",
    "    assert changed_mask.any(), f'nothing changed'\n",
    "    \n",
    "    correct = iy == iyhat\n",
    "\n",
    "    sel = changed_mask\n",
    "    sel = changed_mask & valid\n",
    "    c_pre, c_rec, c_f1, _ = precision_recall_fscore_support(torch.ones_like(correct[sel]).cpu().numpy(), correct[sel].cpu().numpy(), average='binary')\n",
    "    # c_pre, c_rec, c_f1, _ = precision_recall_fscore_support(iy[sel].cpu().numpy(), iyhat[sel].cpu().numpy(), average='binary')\n",
    "    # c_pre, c_rec, c_f1 = 0., 0., 0.\n",
    "    c_acc = correct[sel].float().mean().item()\n",
    "\n",
    "    sel = ~changed_mask\n",
    "    sel = ~changed_mask & valid\n",
    "    u_pre, u_rec, u_f1, _ = precision_recall_fscore_support(iy[sel].cpu().numpy(), iyhat[sel].cpu().numpy(), average='binary')\n",
    "    # u_acc = correct[sel].float().mean().item()\n",
    "    # balanced accuracy = 1/2 * ((TP/(TP+FN)) + (TN/(TN+FP)))\n",
    "    tp = iy[sel] & iyhat[sel]\n",
    "    tn = ~iy[sel] & ~iyhat[sel]\n",
    "    fp = ~iy[sel] & iyhat[sel]\n",
    "    fn = iy[sel] & ~iyhat[sel]\n",
    "    u_acc = 0.5 * ((tp.sum().float() / (tp.sum() + fn.sum())) + (tn.sum().float() / (tn.sum() + fp.sum())))\n",
    "\n",
    "    return c_pre, c_rec, c_f1, c_acc, u_pre, u_rec, u_f1, u_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232db4c3c5a44a09b7a3f0f5e7901a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tr in tqdm(trainers):\n",
    "    x = batch['observation']\n",
    "    y = batch['label']\n",
    "    z = tr.model.encode(x)\n",
    "\n",
    "    score = tr.model.predict(z)\n",
    "    yhat = score > tr.thresholds.unsqueeze(0)\n",
    "\n",
    "    obs_auc = []\n",
    "    obs_pres, obs_recs, obs_f1s = [], [], []\n",
    "    obs_accs = []\n",
    "    for ic in range(num_classes):\n",
    "        auc = roc_auc_score(y[:, ic].cpu().numpy(), score[:, ic].cpu().numpy())\n",
    "        obs_auc.append(auc)\n",
    "        f1 = f1_score(y[:, ic].cpu().numpy(), yhat[:, ic].cpu().numpy())\n",
    "        # pre, rec, f1, _ = precision_recall_fscore_support(y[:, ic].cpu().numpy(), yhat[:, ic].cpu().numpy(), average='binary')\n",
    "        # obs_pres.append(pre.item())\n",
    "        # obs_recs.append(rec.item())\n",
    "        obs_f1s.append(f1.item())\n",
    "        # balanced accuracy\n",
    "        tp = y[:, ic] & yhat[:, ic]\n",
    "        tn = ~y[:, ic] & ~yhat[:, ic]\n",
    "        fp = ~y[:, ic] & yhat[:, ic]\n",
    "        fn = y[:, ic] & ~yhat[:, ic]\n",
    "        acc = 0.5 * ((tp.sum().float() / (tp.sum() + fn.sum())) + (tn.sum().float() / (tn.sum() + fp.sum())))\n",
    "        obs_accs.append(acc.item())\n",
    "    obs_auc = np.array(obs_auc)\n",
    "    # obs_pres = np.array(obs_pres)\n",
    "    # obs_recs = np.array(obs_recs)\n",
    "    obs_f1s = np.array(obs_f1s)\n",
    "    obs_accs = np.array(obs_accs)\n",
    "    # print(obs_auc.mean(), obs_auc.min(), obs_f1s.mean(), obs_f1s.min(), obs_pres.mean(), obs_recs.mean(), obs_sup.mean())\n",
    "    # print(obs_auc.mean(), obs_auc.min(), obs_f1s.mean(), obs_f1s.min())\n",
    "    tr.obs_auc = torch.as_tensor(obs_auc).to(device)\n",
    "    # tr.obs_pres = torch.as_tensor(obs_pres).to(device)\n",
    "    # tr.obs_recs = torch.as_tensor(obs_recs).to(device)\n",
    "    tr.obs_f1s = torch.as_tensor(obs_f1s).to(device)\n",
    "    tr.obs_accs = torch.as_tensor(obs_accs).to(device)\n",
    "\n",
    "    def run_interventions(adds, rems):\n",
    "        if isinstance(tr.intervention, ClassLevelModuleIntervention):\n",
    "            cond = tr.intervention.get_conditioning(adds, rems)\n",
    "            iz = tr.intervention.apply_intervention(z, cond)\n",
    "        else:\n",
    "            iz = tr.intervention.apply_intervention(z, adds, rems)\n",
    "        # iz = Context(tr.intervention, DictGadget({'ambient': z, 'latent_d': z, 'add_class': adds, 'remove_class': rems}))['probe']\n",
    "        iscore = tr.model.predict(iz)\n",
    "        iyhat = iscore > tr.thresholds.unsqueeze(0)\n",
    "        iy = gti.apply_intervention(y, adds, rems)\n",
    "        \n",
    "        c_pre, c_rec, c_f1, c_acc, u_pre, u_rec, u_f1, u_acc = compute_interventional_metrics(y, yhat, iy, iyhat)\n",
    "        return c_pre, c_rec, c_f1, c_acc, u_pre, u_rec, u_f1, u_acc\n",
    "    \n",
    "    torch.manual_seed(11)\n",
    "\n",
    "    num_intv_per_sample = 10\n",
    "\n",
    "    c_accs, u_accs = [], []\n",
    "    c_f1s, c_pres, c_recs, c_supps = [], [], [], []\n",
    "    u_f1s, u_pres, u_recs, u_supps = [], [], [], []\n",
    "    for _ in range(num_intv_per_sample):\n",
    "        adds, rems = None, None\n",
    "        adds = gti.sample_add_intervention(y)\n",
    "        rems = gti.sample_remove_intervention(y)\n",
    "        \n",
    "        c_pre, c_rec, c_f1, c_acc, u_pre, u_rec, u_f1, u_acc = run_interventions(adds, rems)\n",
    "        c_accs.append(c_acc); c_f1s.append(c_f1); c_pres.append(c_pre); c_recs.append(c_rec)\n",
    "        u_accs.append(u_acc); u_f1s.append(u_f1); u_pres.append(u_pre); u_recs.append(u_rec)\n",
    "\n",
    "    tr.itrain_c_acc = torch.as_tensor(c_accs).to(device).mean().item()\n",
    "    tr.itrain_c_f1 = torch.as_tensor(c_f1s).to(device).mean().item()\n",
    "    tr.itrain_u_f1 = torch.as_tensor(u_f1s).to(device).mean().item()\n",
    "    tr.itrain_u_acc = torch.as_tensor(u_accs).to(device).mean().item()\n",
    "\n",
    "\n",
    "    # add interventions (not seen during training)\n",
    "    torch.manual_seed(13)\n",
    "\n",
    "    c_accs, u_accs = [], []\n",
    "    c_f1s, c_pres, c_recs, c_supps = [], [], [], []\n",
    "    u_f1s, u_pres, u_recs, u_supps = [], [], [], []\n",
    "    for _ in range(num_intv_per_sample):\n",
    "        adds = gti.sample_add_intervention(y)\n",
    "        # rems = gti.sample_remove_intervention(y)\n",
    "        \n",
    "        c_pre, c_rec, c_f1, c_acc, u_pre, u_rec, u_f1, u_acc = run_interventions(adds, rems)\n",
    "        c_accs.append(c_acc); c_f1s.append(c_f1); c_pres.append(c_pre); c_recs.append(c_rec)\n",
    "        u_accs.append(u_acc); u_f1s.append(u_f1); u_pres.append(u_pre); u_recs.append(u_rec)\n",
    "\n",
    "    tr.iadd_c_acc = torch.as_tensor(c_accs).to(device).mean().item()\n",
    "    tr.iadd_c_f1 = torch.as_tensor(c_f1s).to(device).mean().item()\n",
    "    tr.iadd_u_f1 = torch.as_tensor(u_f1s).to(device).mean().item()\n",
    "    tr.iadd_u_acc = torch.as_tensor(u_accs).to(device).mean().item()\n",
    "\n",
    "    # remove interventions (not seen during training)\n",
    "    torch.manual_seed(17)\n",
    "\n",
    "    num_intv_per_sample = 10\n",
    "\n",
    "    c_accs, u_accs = [], []\n",
    "    c_f1s, c_pres, c_recs, c_supps = [], [], [], []\n",
    "    u_f1s, u_pres, u_recs, u_supps = [], [], [], []\n",
    "\n",
    "    for _ in range(num_intv_per_sample):\n",
    "        adds, rems = None, None\n",
    "        # adds = gti.sample_add_intervention(y)\n",
    "        rems = gti.sample_remove_intervention(y)\n",
    "        \n",
    "        c_pre, c_rec, c_f1, c_acc, u_pre, u_rec, u_f1, u_acc = run_interventions(adds, rems)\n",
    "        c_accs.append(c_acc); c_f1s.append(c_f1); c_pres.append(c_pre); c_recs.append(c_rec)\n",
    "        u_accs.append(u_acc); u_f1s.append(u_f1); u_pres.append(u_pre); u_recs.append(u_rec)\n",
    "\n",
    "    tr.irem_c_acc = torch.as_tensor(c_accs).to(device).mean().item()\n",
    "    tr.irem_c_f1 = torch.as_tensor(c_f1s).to(device).mean().item()\n",
    "    tr.irem_u_f1 = torch.as_tensor(u_f1s).to(device).mean().item()\n",
    "    tr.irem_u_acc = torch.as_tensor(u_accs).to(device).mean().item()\n",
    "\n",
    "    tr.igen_c_acc = (tr.iadd_c_acc + tr.irem_c_acc) / 2\n",
    "    tr.igen_c_f1 = (tr.iadd_c_f1 + tr.irem_c_f1) / 2\n",
    "    tr.igen_u_acc = (tr.iadd_u_acc + tr.irem_u_acc) / 2\n",
    "    tr.igen_u_f1 = (tr.iadd_u_f1 + tr.irem_u_f1) / 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                  obs-avg-auc    obs-min-auc    obs-avg-f1    obs-min-f1    obs-avg-acc    obs-min-acc    itrain-c-acc    itrain-u-acc    itrain-c-f1    itrain-u-f1    igen-c-acc    igen-u-acc    igen-c-f1    igen-u-f1\n",
      "-------------------  -------------  -------------  ------------  ------------  -------------  -------------  --------------  --------------  -------------  -------------  ------------  ------------  -----------  -----------\n",
      "img2-module               0.938733       0.81007       0.368692     0.0169133       0.875328       0.736687        0.803269        0.933648       0.890899       0.47295       0.493179      0.918613     0.601187     0.437806\n",
      "img2-fromimg-module       0.941325       0.799055      0.37693      0.0538642       0.878109       0.712241        0.707211        0.925563       0.828496       0.301869      0.478037      0.9068       0.613792     0.27934\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([[t.ident, t.obs_auc.mean().item(), t.obs_auc.min().item(), t.obs_f1s.mean().item(), t.obs_f1s.min().item(), \n",
    "                 t.obs_accs.mean().item(), t.obs_accs.min().item(),\n",
    "                 t.itrain_c_acc, t.itrain_u_acc, t.itrain_c_f1, t.itrain_u_f1,\n",
    "                #  t.iadd_c_acc, t.iadd_u_acc, t.iadd_c_f1, t.iadd_u_f1,\n",
    "                #  t.irem_c_acc, t.irem_u_acc, t.irem_c_f1, t.irem_u_f1,\n",
    "                 t.igen_c_acc, t.igen_u_acc, t.igen_c_f1, t.igen_u_f1\n",
    "                 ] for t in trainers], headers=['model', 'obs-avg-auc', 'obs-min-auc', 'obs-avg-f1', 'obs-min-f1', \n",
    "                                                'obs-avg-acc', 'obs-min-acc',\n",
    "                                                'itrain-c-acc', 'itrain-u-acc', 'itrain-c-f1', 'itrain-u-f1',\n",
    "                                                # 'iadd-c-acc', 'iadd-u-acc', 'iadd-c-f1', 'iadd-u-f1',\n",
    "                                                # 'irem-c-acc', 'iadd-u-acc', 'irem-c-f1', 'irem-u-f1', \n",
    "                                                'igen-c-acc', 'igen-u-acc', 'igen-c-f1', 'igen-u-f1'\n",
    "                                                ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model                obs-avg-auc    obs-min-auc    obs-avg-f1    obs-min-f1    obs-avg-acc    obs-min-acc    itrain-c-acc    itrain-u-acc    itrain-c-f1    itrain-u-f1    igen-c-acc    igen-u-acc    igen-c-f1    igen-u-f1\n",
    "# -----------------  -------------  -------------  ------------  ------------  -------------  -------------  --------------  --------------  -------------  -------------  ------------  ------------  -----------  -----------\n",
    "# concept-img             0.962416       0.878639      0.429639     0.028777        0.904266       0.800311        0.894902        0.529389       0.944535      0.0646658      0.947606      0.486482     0.972353    0.0387073\n",
    "# img1-centroid           0.963831       0.884694      0.425504     0.0411311       0.908353       0.79451         0.995684        0.686866       0.997837      0.104248       0.962352      0.705543     0.980513    0.0940431\n",
    "# img1-affine             0.964132       0.885492      0.427893     0.0300501       0.907632       0.78964         0.99483         0.869269       0.997408      0.437516       0.995406      0.891964     0.997698    0.372178\n",
    "# img1-module             0.963528       0.879533      0.426361     0.0428954       0.905049       0.780744        0.745399        0.944005       0.854129      0.395806       0.518853      0.92743      0.653518    0.359271\n",
    "# sem-img1-module         0.964134       0.889826      0.422297     0.0632411       0.908413       0.800464        0.762864        0.94793        0.86548       0.39603        0.489777      0.932898     0.611343    0.361691\n",
    "# sem-img1-centroid       0.963952       0.890605      0.423439     0.045           0.908165       0.786379        0.986376        0.744374       0.993141      0.105887       0.895308      0.737202     0.942253    0.0948086\n",
    "# sem-img1-affine         0.963005       0.88686       0.418932     0.0384615       0.906048       0.798187        0.983091        0.917235       0.991473      0.394326       0.982356      0.920378     0.991099    0.337387\n",
    "\n",
    "# model                 obs-avg-auc    obs-min-auc    obs-avg-f1    obs-min-f1    obs-avg-acc    obs-min-acc    itrain-c-acc    itrain-u-acc    itrain-c-f1    itrain-u-f1    igen-c-acc    igen-u-acc    igen-c-f1    igen-u-f1\n",
    "# ------------------  -------------  -------------  ------------  ------------  -------------  -------------  --------------  --------------  -------------  -------------  ------------  ------------  -----------  -----------\n",
    "# concept-text             0.945094       0.813787      0.397579     0.0167364       0.883867       0.710278        0.881591        0.51173        0.937066      0.04619        0.904767      0.439124     0.949813    0.0316822\n",
    "# text1-centroid           0.951057       0.836843      0.402163     0.04811         0.889618       0.737239        0.826898        0.680325       0.905246      0.0946115      0.701129      0.719687     0.817994    0.0967813\n",
    "# text1-affine             0.950664       0.831114      0.403007     0.0666667       0.890149       0.744445        0.873461        0.872696       0.932456      0.330226       0.763792      0.885963     0.861547    0.287484\n",
    "# text1-module             0.950841       0.843812      0.401961     0.0255941       0.890725       0.753219        0.8167          0.946388       0.899101      0.510001       0.506382      0.926649     0.613678    0.481651\n",
    "# sem-text1-module         0.95062        0.836037      0.413715     0.025641        0.889725       0.746772        0.790387        0.951987       0.882921      0.532646       0.496927      0.928101     0.610615    0.487717\n",
    "# sem-text1-centroid       0.950422       0.837426      0.404985     0.025641        0.889365       0.743982        0.862517        0.926724       0.926181      0.430344       0.58371       0.839321     0.696554    0.263238\n",
    "# sem-text1-affine         0.951138       0.837904      0.402504     0.029661        0.889553       0.762687        0.897039        0.922678       0.945724      0.455885       0.787177      0.914599     0.876818    0.349629\n",
    "\n",
    "# model                  obs-avg-auc    obs-min-auc    obs-avg-f1    obs-min-f1    obs-avg-acc    obs-min-acc    itrain-c-acc    itrain-u-acc    itrain-c-f1    itrain-u-f1    igen-c-acc    igen-u-acc    igen-c-f1    igen-u-f1\n",
    "# -------------------  -------------  -------------  ------------  ------------  -------------  -------------  --------------  --------------  -------------  -------------  ------------  ------------  -----------  -----------\n",
    "# img2-module               0.938733       0.81007       0.368692     0.0169133       0.875328       0.736687        0.803269        0.933648       0.890899       0.47295       0.493179      0.918613     0.601187     0.437806\n",
    "# img2-fromimg-module       0.941325       0.799055      0.37693      0.0538642       0.878109       0.712241        0.707211        0.925563       0.828496       0.301869      0.478037      0.9068       0.613792     0.27934\n",
    "\n",
    "# model                    obs-avg-auc    obs-min-auc    obs-avg-f1    obs-min-f1    obs-avg-acc    obs-min-acc    itrain-c-acc    itrain-u-acc    itrain-c-f1    itrain-u-f1    igen-c-acc    igen-u-acc    igen-c-f1    igen-u-f1\n",
    "# ---------------------  -------------  -------------  ------------  ------------  -------------  -------------  --------------  --------------  -------------  -------------  ------------  ------------  -----------  -----------\n",
    "# text2-module                0.929947       0.782453      0.347344     0.0108342       0.865419       0.686714        0.741108        0.936723       0.851305       0.450485      0.470866      0.921789     0.593254     0.423818\n",
    "# text2-fromtext-module       0.947131       0.834103      0.390687     0.035533        0.886981       0.740133        0.80026         0.945175       0.889048       0.529101      0.505343      0.927797     0.618031     0.494586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
